# Model 02: ADALINE – Empirical Analysis Plan

## 2.1. Backward Link

This document details the practical implementation and performance analysis of the model. For the full historical context and theoretical background, please refer to our **[Theoretical Deep Dive](./01_deep_dive.md)**.

## 2.2. Proposed "Success Case" Experiment

To demonstrate ADALINE’s strengths, we will start with a **classic linearly separable task** where this model should excel. One suitable experiment is to train ADALINE on a **simple 2D linearly separable dataset** generated synthetically (for example, two clusters of points in the plane that can be cleanly separated by a straight line). We can easily create such a dataset using NumPy – for instance, sampling two Gaussian blobs of points, one centered on $(+1, +1)$ and one on $(-1, -1)$, which are separable by the line $x_1 = x_2$ or similar. Each point will have a label: say +1 for the first cluster and -1 for the second. This scenario mirrors the conditions where perceptrons and ADALINE were historically successful (e.g., recognizing simple patterns like a big “X” vs “O” shape that a line can separate).

**Why this dataset?** ADALINE is a *linear* model – it can only form linear decision boundaries. In a linearly separable dataset, there *exists* some straight line (or hyperplane in higher dimensions) that perfectly separates the classes. This means ADALINE, if trained properly, should be able to find a set of weights that achieves near-perfect accuracy. By using a simple synthetic dataset, we eliminate confounding factors and can clearly verify that the model does what it’s supposed to (converges to a correct decision boundary). We could also consider a trivial real dataset such as the logical **AND/OR functions** (which are linearly separable) or a subset of the Iris dataset (taking only two classes that are linearly separable), but the synthetic option gives us full control and clarity.

**Experiment setup:** We will initialize an ADALINE with small random weights and train it on the chosen dataset using the delta rule (stochastic gradient descent). Key parameters like the learning rate and number of epochs will be tuned so that the model converges but doesn’t oscillate. We’ll split the data into training and testing sets to evaluate generalization.

**Expected outcome:** We anticipate that ADALINE will **converge to a solution with very low error**. Specifically, since the data is perfectly separable, the model should reach *zero classification errors* on the training set (or extremely close, limited only by possible learning rate precision issues). The decision boundary learned by ADALINE should align with the true separating line of the data. For example, if we chose two clusters separated by the line $x_1 = x_2$, we expect ADALINE to learn weights that approximate that line (perhaps something like $0.9 x_1 - 0.9 x_2 + b = 0$, which simplifies to $x_1 \approx x_2$, depending on the scale and bias). On the test set (points the model hasn’t seen), we also expect high accuracy, since linear separability means a well-trained linear model should generalize perfectly in theory (barring overfitting issues, which for a linear model in a simple scenario shouldn’t be significant).

**Metrics and visualizations:** To validate success, we will use both quantitative metrics and qualitative visualizations:

* *Accuracy*: We will report the classification accuracy on both the training set and a hold-out test set. In the success case, this should be very high (approaching 100% on both, since no noise or overlap is present in the data).
* *Learning Curve*: We will plot the **mean squared error (MSE)** or sum of squared errors over training epochs. We expect to see the error monotonically decreasing epoch by epoch, asymptotically approaching zero. This visualization will confirm that the delta rule is indeed minimizing the error as expected.
* *Decision Boundary Plot*: We will create a 2D scatter plot of the data points, coloring them by true class, and overlay the **learned decision boundary** (a line in 2D) from the trained ADALINE. This will be a powerful visual confirmation – we should see that the line cleanly separates the two color clusters. If possible, we might also illustrate the margin or distances of points from the boundary, but since ADALINE doesn’t explicitly maximize margin, a simple boundary plot suffices.
* *Weight convergence*: For insight, we could also plot the trajectory of the weight values over training iterations (especially since in 2D we have only two weights + bias). This isn’t a standard metric, but it can be illuminating to show that, say, the weight vector is moving in a direction that corresponds to the true separating line. It’s an optional diagnostic visualization.

By examining these metrics, we should be able to *prove* ADALINE’s success in this case. For instance, a final training MSE approaching zero and a straight-line boundary cleanly partitioning the points will conclusively show that ADALINE has learned the underlying linear pattern. This experiment essentially recreates the kind of scenario where ADALINE was historically used with success (recall that Widrow and Hoff demonstrated ADALINE on problems that were linearly separable or could be made linear via preprocessing). It will validate that our implementation is correct and that the model behaves as theory predicts.

## 2.3. Proposed "Failure Case" Experiment

To probe ADALINE’s limitations, we will test it on a task that **cannot be solved by a linear model** – the classic example is the **XOR problem**. The XOR problem is a two-input binary classification task defined by the truth table: (0,0) → 0, (0,1) → 1, (1,0) → 1, (1,1) → 0. Geometrically, if we take inputs as $(x_1, x_2)$ and treat output as class label, the positive examples (1) are at (0,1) and (1,0), and the negative examples (0) are at (0,0) and (1,1). These four points in the plane are arranged at the corners of a square, with class +1 at two opposite corners and class 0 at the other opposite corners. **There is no single straight line that can separate the +1 points from the 0 points** – any line will end up with one of each class on either side. This is the quintessential example of a non-linearly separable dataset.

**Why this dataset?** XOR is famously the simplest problem that **defeats single-layer perceptrons and ADALINEs**. It was pointed out in Minsky and Papert’s 1969 analysis as a fundamental limitation: a network with no hidden layer cannot represent the XOR function. We choose XOR not only for its historical significance but also because it’s low-dimensional and easy to visualize, making the failure clearly attributable to model capacity rather than, say, lack of data or convergence issues. If ADALINE fails here, we can be confident it’s due to the inherent linear limitation, not a training bug.

**Experiment setup:** We will set up ADALINE with two inputs (plus bias) and train it on the four XOR samples. In fact, to simulate a more “learning” scenario, we might present these four points repeatedly in random order (since a single pass isn’t enough to adjust weights fully). We’ll use the same delta rule update. The target outputs can be encoded as +1 and -1 for the two classes (or 1 and 0; if using +1/-1 it’s symmetric, if 0/1 we’d adapt the error formula slightly, but +1/-1 is convenient for symmetry). We’ll allow the model to train for many epochs and see what happens.

**Expected outcome:** ADALINE will **struggle and ultimately fail to correctly classify XOR**. Specifically, because no linear decision boundary exists that can achieve zero errors, the training process will reach a point where it cannot reduce the error further – a **stalled convergence** at a non-zero error level. We anticipate that the error will bottom out at some relatively high value, and the accuracy will max out at only 75% or 50%. In the XOR problem, the best a linear model can do is, for example, draw a line that classifies three of the four points correctly and one incorrectly (that would be 75% accuracy), or perhaps it oscillates between solutions that each get two of four correct (50% accuracy) depending on initial weights. In practice, ADALINE trained on XOR often finds a compromise: it might adjust weights to minimize squared error by outputting intermediate values. For instance, it could settle in a state where it outputs \~0.5 for all inputs. That would give a mean squared error that’s as low as it can go (since trying to output +1 for two and -1 for two is impossible linearly, it “splits the difference”). But as a classifier, if we apply a threshold at 0, such a solution might classify everything as +1 (or everything as -1), yielding only 50% accuracy. No matter how long we train, ADALINE cannot reach 100% on XOR; the learning will hit a brick wall defined by linear separability.

**Metrics and diagnostics:** To illustrate the failure:

* We will look at the **final training accuracy**, which we expect to be well below 100%. Likely, it will be around 0.5 (if the model just outputs a constant) or 0.75 (if it manages to classify three points correctly and one incorrectly consistently). Either way, it’s significantly worse than the success case.
* The **training error (MSE)** will not go to zero. We will plot the MSE across epochs. What we expect to see is that it decreases initially (as ADALINE finds some partial fit) but then **levels off to a plateau above zero**. It may even oscillate slightly if the learning rate is a bit high, as the algorithm keeps trying to adjust but can’t find a perfect minimum. This behavior – an asymptotic error floor – is a signature that the model’s capacity is insufficient for the task.
* A **decision boundary plot** in the XOR case is very insightful. We can plot the four points and see how ADALINE’s learned boundary is positioned. Likely, it will place a line in an attempt to split the difference. For example, it might put a line that separates one of the +1 points and one of the 0 points from the rest. No matter where the line goes, two points of different classes will lie on the same side of it. By visualizing this, we can identify which inputs it’s misclassifying. We might also plot the actual outputs (before threshold) for each of the four inputs as a way to see what ADALINE is doing – possibly it gives similar scores to certain points because it cannot distinguish them properly.
* We will also inspect the **weights** learned. In a 2D XOR, a linear model’s decision boundary is of the form $w_1 x_1 + w_2 x_2 + b = 0$. The final weights might tell us the orientation of the line it settled on. For instance, it might end up focusing on one of the input bits more than the other, but it doesn’t really matter – no linear combination will crack XOR. By analyzing the weights and outputs, we can confirm there’s no solution and that ADALINE is doing the “best it can” which is still insufficient.

By using the XOR dataset, we expect to **expose the primary limitation of ADALINE**: linear separability. This experiment will likely reproduce what Minsky and Papert pointed out decades ago – that a single-layer network cannot represent the XOR function (or any non-linearly separable function). When we see the training stagnate with a persistent error, it will concretely demonstrate that the model has hit the ceiling of its representational power. The metrics and visualizations listed (accuracy stuck below 100%, error flattening out, a mispositioned decision line) will all serve to diagnose that the failure is due to the *inherent incapability of a linear model to capture the XOR pattern*, rather than a training bug. In essence, this failure case provides a didactic “brick wall” illustrating why we need something more powerful than ADALINE for certain tasks.

## 2.4. The Transition Narrative

The weakness exposed in the failure case is *exactly* the problem that the **MLP** (Multilayer Perceptron) was designed to solve. It addresses this by introducing additional **hidden layer(s) of neurons**, enabling the network to learn **non-linear decision boundaries** that a single-layer ADALINE cannot capture. In particular, an MLP can combine neurons in a second layer to implement functions like XOR (for example, by learning intermediate features that correspond to $x_1 \text{ OR } x_2$ and $x_1 \text{ AND } x_2$ etc.). The MLP’s core innovation – the use of multiple layers with nonlinear activations and a training algorithm called **backpropagation** – allows it to overcome linear separability limitations. By propagating the error back through a hidden layer, the MLP adjusts internal weights to create new feature representations, effectively untangling problems like XOR. In short, where ADALINE fails due to its single-layer linear nature, the MLP succeeds by **building deeper hierarchies of neurons** that can model complex, non-linear relationships. The transition from ADALINE to MLP is a direct response to the former’s shortcomings: **MLPs were introduced to solve exactly those tasks (like XOR) that ADALINE and perceptrons could not solve**, by **introducing non-linearity and multiple adaptive layers** into the network’s architecture.